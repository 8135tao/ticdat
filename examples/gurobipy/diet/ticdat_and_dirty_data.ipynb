{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing dirty data with ticdat\n",
    "\n",
    "Dirty data is an unloved and often overlooked challenge when building analytical models. A typical assumption is that the input data to a model will somehow magically be clean. In reality, there are any number of reasons why dirty data might be passed as input to your engine. The data might be munged together from different systems, the requirements of your data model might be poorly understood, or a user might be simply pushing your model to its limits via what-if analysis. Regardless of the cause, a professional engine will respond gracefully when passed input data that violates basic integrity checks.\n",
    "\n",
    "`ticdat` allows for a data scientist to define data integrity checks for 4 different categories of problems (in addition to checking for the correct table and field names).\n",
    " 1. Duplicate rows (i.e. duplicate primary key entries in the same table).\n",
    " 1. Data type failures. This checks each column for correct data type, legal ranges for numeric data, acceptable flagging strings, nulls present only for columns that allow null, etc.\n",
    " 1. Foreign key failures, which check that each record of a child table can cross-reference into the appropriate parent table.\n",
    " 1. Data predicate failures. This checks each row for conditions more complex than the data type failure checks. For example, a maximum column can not be allowed to be smaller than the minimum column.\n",
    " \n",
    "For a `ticdat` app deployed on Enframe, there will be a dedicated subsection of the input tables dedicated to diagnosing data integrity problems. This subsection is populated whenever an app is solved. There is also an integrity \"Action\" that can be launched to look for data integrity problems independently of the solve process.\n",
    "\n",
    "For a data scientist working offline, `ticdat` provides bulk-query routines that can be used from within a notebook. We briefly tour these routines below. Please consult the docstrings for more information regarding their utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ticdat\n",
    "from diet import input_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we quickly check that the csv files in `diet_sample_data` represent clean data. The `ticdat` bulk query routines all return \"falsey\" results on clean data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = input_schema.csv.create_tic_dat(\"diet_sample_data\")\n",
    "any (_ for _ in [input_schema.csv.find_duplicates(\"diet_sample_data\"),\n",
    "                 input_schema.find_data_type_failures(dat), \n",
    "                 input_schema.find_foreign_key_failures(dat), \n",
    "                 input_schema.find_data_row_failures(dat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine the `diet_dirty_sample_data` data set, which has been deliberately seeded with dirty data.\n",
    "\n",
    "We first check for duplicate rows. Note that since the dict-of-dict format that `TicDat` uses will remove any row duplications when representing a data set in memory, we must check for duplications on the csv files directly. Similar duplication checking routines are provided for all the `TicDatFactory` readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nutrition_quantities': {('milk', 'fat'): 2}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema.csv.find_duplicates(\"diet_dirty_sample_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ticdat` is telling us that there are two different records in the Nutrition Quantities table defining the amount of fat in milk. This can be easily confirmed by manually inspecting the \"Nutrition Quantities.csv\" file in the \"diet_dirty_sample_data\" directory. In a real-world data set, manual inspection would be impossible and such a duplication would be easily overlooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TableField(table='nutrition_quantities', field='Quantity'): ValuesPks(bad_values=(None,), pks=(('macaroni', 'calories'), ('chicken', 'fat')))}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = input_schema.csv.create_tic_dat(\"diet_dirty_sample_data\")\n",
    "input_schema.find_data_type_failures(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('nutrition_quantities', 'Quantity'): (('macaroni', 'calories'),\n",
       "  ('chicken', 'fat'))}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{tuple(k): v.pks for k, v in input_schema.find_data_type_failures(dat).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ticdat` is telling us that there are two rows which have bad values in the Quantity field of the Nutrition Quantities table. In both cases, the problem is `None` (or equivalently a Null) where a number is expected. The rows with this problem are those which specify the quantity for `('macaroni', 'calories')` and `('chicken', 'fat')`. As before, these two errant rows can easily be double checked by manually examining \"Nutrition Quantities.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('nutrition_quantities', 'foods', ('Food', 'Name')): (('pizza',),\n",
       "  (('pizza', 'sodium'),\n",
       "   ('pizza', 'fat'),\n",
       "   ('pizza', 'calories'),\n",
       "   ('pizza', 'protein')))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema.find_foreign_key_failures(dat, verbosity=\"Low\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `ticdat` is telling us that there are 4 records in the Nutrition Quantities table that fail to cross reference with the Foods table. In all 4 cases, it is specifically the \"pizza\" string in the Food field that fails to find a match from the Name field of the Foods table. If you manually examine \"Foods.csv\", you can see this problem arose because of the Foods table was altered to have a \"pizza pie\" entry instead of a \"pizza\" entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TablePredicateName(table='categories', predicate_name='Min Max Check'): ('fat',)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema.find_data_row_failures(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `ticdat` is telling us that the \"Min Max Check\" (i.e. the check that `row[\"Max Nutrition\"] >= row[\"Min Nutrition\"]`) failed for the \"fat\" record of the Categories table. This is easily verified by manual inspection of \"Foods.csv\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
